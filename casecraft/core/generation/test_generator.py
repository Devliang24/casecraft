"""Test case generator using LLM."""

import json
import os
import asyncio
from typing import Any, Dict, List, Optional
from dataclasses import dataclass

from jsonschema import ValidationError, validate
from pydantic import ValidationError as PydanticValidationError

from casecraft.core.generation.llm_client import LLMClient, LLMError
from casecraft.core.parsing.headers_analyzer import HeadersAnalyzer
from casecraft.core.analysis import PathAnalyzer, SmartDescriptionGenerator, CriticalityAnalyzer
from casecraft.core.analysis.constants import METHOD_BASE_COUNTS, COMPLEXITY_THRESHOLDS, TEST_TYPE_RATIOS, MIN_TEST_COUNTS, MAX_TEST_COUNTS
from casecraft.models.api_spec import APIEndpoint
from casecraft.models.test_case import TestCase, TestCaseCollection, TestType
from casecraft.models.usage import TokenUsage
from casecraft.utils.logging import CaseCraftLogger


class TestGeneratorError(Exception):
    """Test generation related errors."""
    pass


@dataclass
class GenerationResult:
    """Result of test case generation including token usage."""
    
    test_cases: TestCaseCollection
    token_usage: Optional[TokenUsage] = None
    retry_count: int = 0  # Number of retries performed


class TestCaseGenerator:
    """Generates test cases for API endpoints using LLM."""
    
    def __init__(self, llm_client: LLMClient, api_version: Optional[str] = None, console=None):
        """Initialize test case generator.
        
        Args:
            llm_client: LLM client instance
            api_version: API version string
            console: Optional Rich console for output (helps with progress bar coordination)
        """
        self.llm_client = llm_client
        self.api_version = api_version
        self.headers_analyzer = HeadersAnalyzer()
        self.logger = CaseCraftLogger("test_generator", console=console, show_timestamp=True, show_level=True)
        self._test_case_schema = self._get_test_case_schema()
        
        # åˆå§‹åŒ–æ™ºèƒ½åˆ†æžå™¨
        self.path_analyzer = PathAnalyzer()
        self.description_generator = SmartDescriptionGenerator()
        self.criticality_analyzer = CriticalityAnalyzer()
    
    def _generate_concise_chinese_description(self, endpoint: APIEndpoint) -> str:
        """Generate concise Chinese description for endpoint using smart inference.
        
        Args:
            endpoint: API endpoint
            
        Returns:
            Concise Chinese description
        """
        # ä½¿ç”¨æ™ºèƒ½æè¿°ç”Ÿæˆå™¨æ›¿ä»£ç¡¬ç¼–ç 
        return self.description_generator.generate(endpoint)
    
    async def generate_test_cases(self, endpoint: APIEndpoint, progress_callback: Optional[callable] = None) -> GenerationResult:
        """Generate test cases for an API endpoint with intelligent retry mechanism.
        
        Args:
            endpoint: API endpoint to generate tests for
            progress_callback: Optional callback for progress updates
            
        Returns:
            Generation result including test cases and token usage information
            
        Raises:
            TestGeneratorError: If test generation fails after all retries
        """
        max_attempts = 3
        last_error = None
        total_tokens_used = 0
        endpoint_id = endpoint.get_endpoint_id()
        
        # Log start of generation
        self.logger.file_only(f"ðŸ”¨ Preparing test generation for {endpoint_id}")
        
        # Analyze endpoint complexity
        complexity = self._evaluate_endpoint_complexity(endpoint)
        self.logger.file_only(f"Endpoint complexity: score={complexity['complexity_score']}, level={complexity['complexity_level']}", level="DEBUG")
        
        for attempt in range(max_attempts):
            try:
                # Build prompt - use enhanced version for retries
                if attempt > 0 and last_error:
                    # Log retry attempt
                    retry_msg = f"Retry attempt {attempt + 1}/{max_attempts} for {endpoint_id}"
                    self.logger.info(retry_msg)
                    prompt = self._build_prompt_with_retry_hints(endpoint, last_error, attempt)
                    system_prompt = self._get_system_prompt_with_retry_emphasis()
                else:
                    self.logger.file_only(f"Building prompt for {endpoint_id}", level="DEBUG")
                    prompt = self._build_prompt(endpoint)
                    system_prompt = self._get_system_prompt()
                
                # Log prompt info
                prompt_tokens = len(prompt) // 4  # Rough estimate
                self.logger.file_only(f"Prompt prepared: ~{prompt_tokens} tokens", level="DEBUG")
                
                # Generate with streaming support
                # Check if streaming is enabled (handle both provider and legacy config modes)
                is_streaming = False
                if hasattr(self.llm_client, 'provider') and self.llm_client.provider:
                    is_streaming = getattr(self.llm_client.provider.config, 'stream', False)
                elif hasattr(self.llm_client, 'config') and self.llm_client.config:
                    is_streaming = getattr(self.llm_client.config, 'stream', False)
                
                if is_streaming and attempt == 0:  # Only show streaming on first attempt
                    self.logger.file_only(f"Using streaming mode for {endpoint.get_endpoint_id()}")
                
                # Create enhanced retry-aware progress callback
                retry_aware_callback = None
                if progress_callback:
                    def retry_aware_callback(stream_progress: float, http_retry_info: Optional[Dict[str, Any]] = None):
                        # Enhanced retry info with more detail
                        retry_info = None
                        if attempt > 0 or http_retry_info:
                            retry_info = {
                                'generation_retry': {
                                    'current': attempt + 1 if attempt > 0 else 0,
                                    'total': max_attempts,
                                    'attempt_phase': 'generation',
                                    'last_error': str(last_error)[:50] + '...' if last_error and attempt > 0 else None
                                }
                            }
                            if http_retry_info:
                                retry_info['http_retry'] = http_retry_info
                        
                        # Use non-blocking progress update
                        try:
                            progress_callback(stream_progress, retry_info)
                        except Exception as e:
                            # Don't let progress callback errors break generation
                            self.logger.warning(f"Progress callback error: {e}")
                
                # Phase progress updates for better user feedback
                if progress_callback:
                    try:
                        phase_progress = 0.1 + (attempt * 0.1)  # Each retry starts from higher base
                        retry_aware_callback(phase_progress, None)  # Signal generation start
                    except Exception:
                        pass  # Ignore progress callback errors
                
                response = await self.llm_client.generate(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    progress_callback=retry_aware_callback
                )
                
                # Track token usage across retries
                self.logger.file_only(f"LLM response.usage: {response.usage}", level="DEBUG")
                if response.usage:
                    total_tokens_used += response.usage.get("total_tokens", 0)
                    self.logger.file_only(f"Added tokens, total now: {total_tokens_used}", level="DEBUG")
                
                # Validate response format early
                self._validate_response_format(response.content)
                
                # Parse and validate LLM response
                test_cases = self._parse_llm_response(response.content, endpoint)
                
                # Enhance test cases with response schemas and smart status codes
                test_cases = self._enhance_test_cases(test_cases, endpoint)
                
                # Generate concise Chinese description
                concise_description = self._generate_concise_chinese_description(endpoint)
                
                # Create test case collection with metadata
                collection = TestCaseCollection(
                    endpoint_id=endpoint.get_endpoint_id(),
                    method=endpoint.method,
                    path=endpoint.path,
                    summary=endpoint.summary,
                    description=concise_description,
                    test_cases=test_cases
                )
                
                # Set metadata at collection level
                collection.metadata.llm_model = response.model
                collection.metadata.api_version = self.api_version
                
                # Extract token usage from LLM response (use total from all attempts)
                token_usage = None
                self.logger.file_only(f"Response usage: {response.usage}, total_tokens_used: {total_tokens_used}", level="DEBUG")
                if response.usage or total_tokens_used > 0:
                    token_usage = TokenUsage(
                        prompt_tokens=response.usage.get("prompt_tokens", 0) if response.usage else 0,
                        completion_tokens=response.usage.get("completion_tokens", 0) if response.usage else 0,
                        total_tokens=response.usage.get("total_tokens", 0) if response.usage else 0,
                        model=response.model,
                        endpoint_id=endpoint.get_endpoint_id(),
                        retry_count=attempt  # Record actual attempts made
                    )
                    self.logger.file_only(f"Created TokenUsage: {token_usage}")
                
                # Success! Log generation details
                test_count = len(collection.test_cases)
                positive_count = sum(1 for tc in collection.test_cases if tc.test_type == "positive")
                negative_count = sum(1 for tc in collection.test_cases if tc.test_type == "negative")
                boundary_count = sum(1 for tc in collection.test_cases if tc.test_type == "boundary")
                
                if attempt > 0:
                    self.logger.file_only(f"âœ¨ Successfully generated {test_count} test cases on attempt {attempt + 1} for {endpoint_id}")
                else:
                    self.logger.file_only(f"âœ¨ Successfully generated {test_count} test cases for {endpoint_id}")
                
                self.logger.file_only(f"Test case breakdown: {positive_count} positive, {negative_count} negative, {boundary_count} boundary", level="DEBUG")
                
                return GenerationResult(
                    test_cases=collection,
                    retry_count=attempt,  # Add retry count to result
                    token_usage=token_usage
                )
                
            except (TestGeneratorError, ValidationError, json.JSONDecodeError) as e:
                last_error = str(e)
                self.logger.warning(f"Attempt {attempt + 1} failed for {endpoint.get_endpoint_id()}: {e}")
                
                # Check if we should retry
                if self._should_retry(e) and attempt < max_attempts - 1:
                    self.logger.info(f"Will retry with enhanced prompt for {endpoint.get_endpoint_id()}")
                    await asyncio.sleep(2)  # Brief delay before retry
                    continue
                else:
                    # Final failure - create error with retry statistics
                    error_msg = f"Failed to generate test cases for {endpoint.get_endpoint_id()} after {attempt + 1} attempts: {e}"
                    # Only log as error if it's the final failure after all retries
                    if attempt == max_attempts - 1:
                        self.logger.error(error_msg)
                    else:
                        self.logger.warning(error_msg)
                    
                    # Import here to avoid circular imports
                    from casecraft.core.providers.exceptions import ProviderError
                    
                    # Create error with detailed retry statistics
                    retry_error = ProviderError.create_with_retry_stats(
                        message=error_msg,
                        provider_name="TestGenerator",
                        generation_retries=attempt + 1,
                        generation_max_retries=max_attempts,
                        retry_reasons=[str(e) for e in [last_error] if e]
                    )
                    raise TestGeneratorError(retry_error.get_friendly_message())
            
            except Exception as e:
                # Unexpected error - don't retry, but still include basic retry info
                from casecraft.core.providers.exceptions import ProviderError
                
                retry_error = ProviderError.create_with_retry_stats(
                    message=f"Unexpected error generating test cases for {endpoint.get_endpoint_id()}: {e}",
                    provider_name="TestGenerator",
                    generation_retries=attempt + 1,
                    generation_max_retries=max_attempts,
                    retry_reasons=["Unexpected error"]
                )
                raise TestGeneratorError(retry_error.get_friendly_message())
    
    def _should_retry(self, error: Exception) -> bool:
        """Determine if error is worth retrying.
        
        Args:
            error: The exception that occurred
            
        Returns:
            True if should retry, False otherwise
        """
        error_str = str(error).lower()
        
        # Retry for format/validation errors
        retry_patterns = [
            "validation error",
            "required property",
            "invalid json",
            "failed to parse",
            "test case",
            "at least",
            "header",
            "instead of"
        ]
        
        # Don't retry for these errors
        no_retry_patterns = [
            "authentication",
            "unauthorized",
            "api key",
            "model not found",
            "rate limit",
            "quota"
        ]
        
        # Check if it's a no-retry error first
        for pattern in no_retry_patterns:
            if pattern in error_str:
                return False
        
        # Check if it's a retry-worthy error
        for pattern in retry_patterns:
            if pattern in error_str:
                return True
        
        # Default to retry for unknown errors
        return isinstance(error, (TestGeneratorError, ValidationError, json.JSONDecodeError))
    
    def _validate_response_format(self, content: str) -> None:
        """Validate response format early to catch common errors.
        
        Args:
            content: Response content from LLM
            
        Raises:
            TestGeneratorError: If response format is invalid
        """
        try:
            # Try to parse JSON first
            parsed = json.loads(content)
            
            # Check if it's a dict that looks like headers
            if isinstance(parsed, dict) and not isinstance(parsed, list):
                # Check for common header keys
                header_keys = ['content-type', 'accept', 'authorization', 'user-agent']
                if any(key.lower() in [k.lower() for k in parsed.keys()] for key in header_keys):
                    raise TestGeneratorError(
                        "LLM returned headers object instead of test cases array. "
                        "Expected format: [{test_case_1}, {test_case_2}, ...]"
                    )
                
                # If it's a single test case, we'll handle it in parsing
                if 'test_id' in parsed or 'name' in parsed:
                    self.logger.file_only("Response is a single test case object, will wrap in array", level="WARNING")
            
            # Check if it's an empty response
            if not parsed:
                raise TestGeneratorError("LLM returned empty response")
                
        except json.JSONDecodeError as e:
            # Let the main parser handle this
            self.logger.debug(f"JSON decode error in format validation: {e}")
    
    def _build_prompt_with_retry_hints(self, endpoint: APIEndpoint, last_error: str, attempt: int) -> str:
        """Build enhanced prompt with retry hints based on previous error.
        
        Args:
            endpoint: API endpoint
            last_error: Error message from last attempt
            attempt: Current attempt number
            
        Returns:
            Enhanced prompt string
        """
        base_prompt = self._build_prompt(endpoint)
        
        # Analyze error to provide specific hints
        error_hints = []
        
        if "required property" in last_error or "test_id" in last_error:
            error_hints.append("æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»åŒ…å«: test_id, name, description, method, path, status, test_type")
        
        if "headers" in last_error.lower() and "instead of" in last_error:
            error_hints.append("ä¸è¦åªè¿”å›žheadersï¼Œå¿…é¡»è¿”å›žå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹æ•°ç»„")
        
        if "json" in last_error.lower():
            error_hints.append("ç¡®ä¿è¿”å›žæœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼")
        
        # Parse specific count requirements from error message
        if "at least" in last_error:
            import re
            # Try to extract specific numbers from error message
            # Pattern: "At least X positive/negative/boundary test cases required, got Y"
            pattern = r"At least (\d+) (\w+) test cases? (?:are )?required.*got (\d+)"
            match = re.search(pattern, last_error)
            if match:
                required_count = match.group(1)
                test_type = match.group(2)
                actual_count = match.group(3)
                error_hints.append(f"å¼ºçƒˆå»ºè®®ç”Ÿæˆ {required_count} ä¸ªæˆ–æ›´å¤š {test_type} ç±»åž‹çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆå½“å‰åªæœ‰ {actual_count} ä¸ªï¼‰ä»¥ç¡®ä¿å……åˆ†è¦†ç›–")
            else:
                # Fallback generic hint
                error_hints.append("å»ºè®®ç”ŸæˆæŽ¨èæ•°é‡çš„æµ‹è¯•ç”¨ä¾‹ä»¥ç¡®ä¿å…¨é¢æµ‹è¯•è¦†ç›–")
            
            # Also get the complexity requirements for this endpoint
            complexity = self._evaluate_endpoint_complexity(endpoint)
            error_hints.append(f"è¯¥ {complexity['complexity_level']} å¤æ‚åº¦ç«¯ç‚¹éœ€è¦ï¼š")
            error_hints.append(f"  â€¢ æ­£å‘æµ‹è¯•: {complexity['recommended_counts']['positive'][0]}-{complexity['recommended_counts']['positive'][1]} ä¸ª")
            error_hints.append(f"  â€¢ è´Ÿå‘æµ‹è¯•: {complexity['recommended_counts']['negative'][0]}-{complexity['recommended_counts']['negative'][1]} ä¸ª")
            error_hints.append(f"  â€¢ è¾¹ç•Œæµ‹è¯•: {complexity['recommended_counts']['boundary'][0]}-{complexity['recommended_counts']['boundary'][1]} ä¸ª")
            error_hints.append(f"  â€¢ æ€»è®¡: {complexity['recommended_counts']['total'][0]}-{complexity['recommended_counts']['total'][1]} ä¸ª")
        
        # Build retry hint section
        # Get complexity info for better examples
        complexity = self._evaluate_endpoint_complexity(endpoint)
        min_positive = complexity['recommended_counts']['positive'][0]
        min_negative = complexity['recommended_counts']['negative'][0]
        
        retry_hint = f"""

âš ï¸ **é‡è¯• {attempt + 1}/3 - ä¸Šæ¬¡ç”Ÿæˆå¤±è´¥**

é”™è¯¯ä¿¡æ¯: {last_error[:200]}

**ç‰¹åˆ«æ³¨æ„äº‹é¡¹:**
{chr(10).join(f"â€¢ {hint}" for hint in error_hints)}

**æŽ¨èçš„è¿”å›žæ ¼å¼ç¤ºä¾‹ï¼ˆå»ºè®®ç”Ÿæˆ {min_positive} ä¸ªæ­£å‘ + {min_negative} ä¸ªè´Ÿå‘æµ‹è¯•ï¼Œå…¨é¢è¦†ç›–æ›´é‡è¦ï¼‰:**
```json
[
  {{
    "test_id": 1,
    "name": "æˆåŠŸåˆ›å»ºç”¨æˆ·",
    "description": "æµ‹è¯•æ­£å¸¸çš„ç”¨æˆ·æ³¨å†Œæµç¨‹",
    "method": "{endpoint.method}",
    "path": "{endpoint.path}",
    "headers": {{"Content-Type": "application/json"}},
    "body": {{"username": "test", "password": "123456"}},
    "status": 200,
    "test_type": "positive"
  }},
  {{
    "test_id": 2,
    "name": "åŒ…å«å¯é€‰å­—æ®µçš„æˆåŠŸè¯·æ±‚",
    "description": "æµ‹è¯•åŒ…å«æ‰€æœ‰å¯é€‰å­—æ®µçš„æƒ…å†µ",
    "method": "{endpoint.method}",
    "path": "{endpoint.path}",
    "headers": {{"Content-Type": "application/json"}},
    "body": {{"username": "test2", "password": "123456", "email": "test@example.com"}},
    "status": 200,
    "test_type": "positive"
  }},
  {{
    "test_id": 3,
    "name": "ç¼ºå°‘å¿…éœ€å­—æ®µ",
    "description": "æµ‹è¯•ç¼ºå°‘usernameå­—æ®µçš„æƒ…å†µ",
    "method": "{endpoint.method}",
    "path": "{endpoint.path}",
    "headers": {{"Content-Type": "application/json"}},
    "body": {{"password": "123456"}},
    "status": 400,
    "test_type": "negative"
  }},
  {{
    "test_id": 4,
    "name": "æ— æ•ˆçš„å‚æ•°æ ¼å¼",
    "description": "æµ‹è¯•å‚æ•°æ ¼å¼é”™è¯¯çš„æƒ…å†µ",
    "method": "{endpoint.method}",
    "path": "{endpoint.path}",
    "headers": {{"Content-Type": "application/json"}},
    "body": {{"username": 123, "password": "123456"}},
    "status": 400,
    "test_type": "negative"
  }}
]
```

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç¡®ä¿è¿”å›žJSONæ•°ç»„ã€‚
ðŸ’¡ **å»ºè®®**ï¼šæŽ¨èç”Ÿæˆå……åˆ†çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆ{min_positive}+ æ­£å‘ï¼Œ{min_negative}+ è´Ÿå‘ï¼‰ä»¥ç¡®ä¿å…¨é¢æµ‹è¯•è¦†ç›–ã€‚è´¨é‡å’Œæ•°é‡åŒæ ·é‡è¦ï¼
"""
        
        return base_prompt + retry_hint
    
    def _get_system_prompt_with_retry_emphasis(self) -> str:
        """Get enhanced system prompt for retry attempts."""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„APIç”¨ä¾‹è®¾è®¡å·¥ç¨‹å¸ˆã€‚

âš ï¸ é‡è¦æé†’ï¼ˆé‡è¯•æ—¶å¿…é¡»æ³¨æ„ï¼‰ï¼š
1. å¿…é¡»è¿”å›žJSONæ•°ç»„æ ¼å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹ä¹Ÿè¦ç”¨ [...] åŒ…è£…
2. æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½å¿…é¡»åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼ˆtest_id, name, description, method, path, status, test_typeï¼‰
3. ä¸è¦åªè¿”å›žheadersæˆ–å…¶ä»–éƒ¨åˆ†å†…å®¹ï¼Œå¿…é¡»è¿”å›žå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹å¯¹è±¡
4. ä¸¥æ ¼éµå®ˆJSONè¯­æ³•ï¼Œç¡®ä¿å¯ä»¥è¢«æ­£ç¡®è§£æž
5. æŽ¨èç”Ÿæˆå……åˆ†æ•°é‡çš„æµ‹è¯•ç”¨ä¾‹æ¥ç¡®ä¿å…¨é¢è¦†ç›–ï¼š
   - simpleç«¯ç‚¹ï¼šå»ºè®®3ä¸ªpositive + 3ä¸ªnegativeï¼ˆæœ€å°‘2ä¸ªpositive + 2ä¸ªnegativeï¼‰
   - mediumç«¯ç‚¹ï¼šå»ºè®®4ä¸ªpositive + 4ä¸ªnegativeï¼ˆæœ€å°‘3ä¸ªpositive + 3ä¸ªnegativeï¼‰ 
   - complexç«¯ç‚¹ï¼šå»ºè®®5ä¸ªpositive + 5ä¸ªnegativeï¼ˆæœ€å°‘4ä¸ªpositive + 4ä¸ªnegativeï¼‰
6. æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»æœ‰æ˜Žç¡®çš„æµ‹è¯•ç›®çš„ï¼Œè¦†ç›–ä¸åŒåœºæ™¯
7. å…¨é¢çš„æµ‹è¯•è¦†ç›–æ¯”èŠ‚çœtokenæ›´é‡è¦

æ ¹æ®æä¾›çš„APIè§„èŒƒå’Œå¤æ‚åº¦è¦æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚è¯·ç”ŸæˆæŽ¨èæ•°é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œç¡®ä¿å…¨é¢è¦†ç›–å„ç§æ­£å‘å’Œè´Ÿå‘åœºæ™¯ï¼"""
    
    def _get_system_prompt(self) -> str:
        """Get system prompt for LLM."""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„APIç”¨ä¾‹è®¾è®¡å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£è®¾è®¡å…¨é¢ã€é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚

âš ï¸ **å…³é”®è¦æ±‚ï¼šå¿…é¡»ç”Ÿæˆå……è¶³æ•°é‡çš„æ­£å‘æµ‹è¯•ç”¨ä¾‹ï¼**
- POSTç«¯ç‚¹ï¼šè‡³å°‘7ä¸ªæ­£å‘æµ‹è¯•ç”¨ä¾‹
- GETç«¯ç‚¹ï¼šè‡³å°‘4ä¸ªæ­£å‘æµ‹è¯•ç”¨ä¾‹  
- PUT/PATCHç«¯ç‚¹ï¼šè‡³å°‘5ä¸ªæ­£å‘æµ‹è¯•ç”¨ä¾‹
- DELETEç«¯ç‚¹ï¼šè‡³å°‘4ä¸ªæ­£å‘æµ‹è¯•ç”¨ä¾‹

## ðŸŽ¯ æ ¸å¿ƒè®¾è®¡ç†å¿µ
"å®Œå–„çš„æµ‹è¯•è®¾è®¡æ˜¯é«˜è´¨é‡APIçš„åŸºçŸ³" - è¯·è®¾è®¡å……åˆ†çš„æµ‹è¯•ç”¨ä¾‹ä»¥ç¡®ä¿æŽ¥å£çš„å¯é æ€§ã€‚

## ðŸ“Š æµ‹è¯•æ•°é‡æŒ‡å¯¼åŽŸåˆ™

æ ¹æ®HTTPæ–¹æ³•çš„é‡è¦æ€§å’Œé£Žé™©ç­‰çº§ï¼Œè®¾è®¡ç›¸åº”æ•°é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼š

### POSTï¼ˆåˆ›å»ºæ“ä½œï¼‰- æœ€é‡è¦
**ç›®æ ‡æ•°é‡ï¼š16-25ä¸ªæµ‹è¯•ç”¨ä¾‹**
- æ­£å‘æµ‹è¯•ï¼ˆ35%ï¼‰ï¼š6-9ä¸ª
- è´Ÿå‘æµ‹è¯•ï¼ˆ45%ï¼‰ï¼š7-11ä¸ª
- è¾¹ç•Œæµ‹è¯•ï¼ˆ20%ï¼‰ï¼š3-5ä¸ª
åˆ›å»ºæ“ä½œå½±å“æ•°æ®å®Œæ•´æ€§ï¼Œéœ€è¦æœ€å…¨é¢çš„æµ‹è¯•ã€‚

**POSTå¿…é¡»åŒ…å«çš„æµ‹è¯•åœºæ™¯ï¼š**
â–¡ è®¤è¯æŽˆæƒæµ‹è¯•ï¼ˆæœªè®¤è¯401ã€æ— æƒé™403ã€tokenè¿‡æœŸ401ï¼‰
â–¡ å¹¶å‘åˆ›å»ºå¤„ç†ï¼ˆåŒæ—¶åˆ›å»ºç›¸åŒèµ„æºã€ä¸åŒèµ„æºï¼‰
â–¡ ä¸šåŠ¡è§„åˆ™éªŒè¯ï¼ˆåº“å­˜ä¸è¶³ã€å•†å“ä¸‹æž¶ã€è¶…å‡ºé™åˆ¶ï¼‰
â–¡ å”¯ä¸€æ€§çº¦æŸï¼ˆé‡å¤åˆ›å»º409ã€å”¯ä¸€å­—æ®µå†²çªï¼‰
â–¡ æ•°æ®å®Œæ•´æ€§ï¼ˆå¤–é”®çº¦æŸã€å¼•ç”¨éªŒè¯ï¼‰
â–¡ äº‹åŠ¡å¤„ç†ï¼ˆéƒ¨åˆ†å¤±è´¥å›žæ»šã€æ‰¹é‡åˆ›å»ºï¼‰
â–¡ èµ„æºé™åˆ¶ï¼ˆé…é¢é™åˆ¶ã€é€ŸçŽ‡é™åˆ¶429ï¼‰

### DELETEï¼ˆåˆ é™¤æ“ä½œï¼‰- ç¬¬äºŒé‡è¦
**ç›®æ ‡æ•°é‡ï¼š15-22ä¸ªæµ‹è¯•ç”¨ä¾‹**
- æ­£å‘æµ‹è¯•ï¼ˆ25%ï¼‰ï¼š4-6ä¸ª
- è´Ÿå‘æµ‹è¯•ï¼ˆ55%ï¼‰ï¼š8-12ä¸ª
- è¾¹ç•Œæµ‹è¯•ï¼ˆ20%ï¼‰ï¼š3-4ä¸ª
åˆ é™¤æ“ä½œä¸å¯é€†ï¼Œå¿…é¡»å……åˆ†æµ‹è¯•æƒé™ã€å­˜åœ¨æ€§ã€çº§è”å½±å“ã€‚

### PUT/PATCHï¼ˆæ›´æ–°æ“ä½œï¼‰
**ç›®æ ‡æ•°é‡ï¼š14-20ä¸ªæµ‹è¯•ç”¨ä¾‹**
- æ­£å‘æµ‹è¯•ï¼ˆ35%ï¼‰ï¼š5-7ä¸ª
- è´Ÿå‘æµ‹è¯•ï¼ˆ45%ï¼‰ï¼š6-9ä¸ª
- è¾¹ç•Œæµ‹è¯•ï¼ˆ20%ï¼‰ï¼š3-4ä¸ª

### GETï¼ˆæŸ¥è¯¢æ“ä½œï¼‰
**ç›®æ ‡æ•°é‡ï¼š13-20ä¸ªæµ‹è¯•ç”¨ä¾‹**
- æ­£å‘æµ‹è¯•ï¼ˆ40%ï¼‰ï¼š5-8ä¸ª
- è´Ÿå‘æµ‹è¯•ï¼ˆ40%ï¼‰ï¼š5-8ä¸ª
- è¾¹ç•Œæµ‹è¯•ï¼ˆ20%ï¼‰ï¼š3-4ä¸ª

## ðŸ“ DELETEæ“ä½œç‰¹æ®Šæµ‹è¯•è¦æ±‚ï¼ˆé‡è¦ï¼‰
å¿…é¡»åŒ…å«ä»¥ä¸‹æµ‹è¯•åœºæ™¯ï¼š
â–¡ åˆ é™¤ä¸å­˜åœ¨çš„èµ„æºï¼ˆ404ï¼‰
â–¡ é‡å¤åˆ é™¤åŒä¸€èµ„æºï¼ˆ404æˆ–409ï¼‰
â–¡ åˆ é™¤è¢«å¼•ç”¨çš„èµ„æºï¼ˆ409å†²çªï¼‰
â–¡ çº§è”åˆ é™¤éªŒè¯ï¼ˆéªŒè¯å…³è”æ•°æ®å¤„ç†ï¼‰
â–¡ è½¯åˆ é™¤vsç¡¬åˆ é™¤ï¼ˆæ ‡è®°åˆ é™¤vsç‰©ç†åˆ é™¤ï¼‰
â–¡ åˆ é™¤æƒé™éªŒè¯ï¼ˆ401æœªè®¤è¯ï¼Œ403æ— æƒé™ï¼‰
â–¡ åˆ é™¤åŽçš„æ•°æ®æ¢å¤ï¼ˆéªŒè¯æ˜¯å¦å¯æ¢å¤ï¼‰
â–¡ æ‰¹é‡åˆ é™¤åœºæ™¯ï¼ˆåˆ é™¤å¤šä¸ªèµ„æºï¼‰
â–¡ å¹¶å‘åˆ é™¤å¤„ç†ï¼ˆåŒæ—¶åˆ é™¤åŒä¸€èµ„æºï¼‰
â–¡ åˆ é™¤é”å®šçš„èµ„æºï¼ˆ423é”å®šï¼‰

æµ‹è¯•ç”¨ä¾‹è¦æ±‚ï¼š
- æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»æœ‰test_idï¼ˆä»Ž1å¼€å§‹çš„é€’å¢žç¼–å·ï¼‰
- ä½¿ç”¨ä¸­æ–‡å‘½åï¼Œnameå’Œdescriptionéƒ½ç”¨ä¸­æ–‡æè¿°
- é€‰æ‹©åˆé€‚çš„çŠ¶æ€ç ï¼š200(æˆåŠŸ)ã€400(å‚æ•°é”™è¯¯)ã€404(èµ„æºä¸å­˜åœ¨)ã€422(éªŒè¯å¤±è´¥)ã€401(æœªè®¤è¯)ã€403(æ— æƒé™)
- æµ‹è¯•æ•°æ®è¦çœŸå®žä¸”ç®€çŸ­
- ç¡®ä¿æµ‹è¯•ç”¨ä¾‹å…·æœ‰å®žé™…æ„ä¹‰ï¼Œé¿å…é‡å¤æˆ–æ— æ•ˆçš„æµ‹è¯•

è´¨é‡ä¸Žæ•°é‡å¹¶é‡ï¼š
- ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒæ—¶ç¡®ä¿å……åˆ†çš„æ•°é‡
- å…¨é¢çš„æµ‹è¯•æ¯”èŠ‚çœtokenæ›´é‡è¦
- ç›®æ ‡æ˜¯è¾¾åˆ°æŽ¨èæ•°é‡ï¼Œè€Œä¸æ˜¯æœ€ä½Žè¦æ±‚
- å¤šæ ·åŒ–çš„æµ‹è¯•åœºæ™¯èƒ½å‘çŽ°æ›´å¤šæ½œåœ¨é—®é¢˜

Headersè®¾ç½®æ™ºèƒ½è§„åˆ™ï¼š
1. åŸºäºŽHTTPæ–¹æ³•çš„Headersï¼ˆé‡è¦ï¼šå¿…é¡»å®Œæ•´ï¼Œä¸è¦æˆªæ–­ï¼‰ï¼š
   - GET: æ·»åŠ  "Accept": "application/json" ï¼ˆå®Œæ•´çš„MIMEç±»åž‹ï¼‰
   - POST/PUT/PATCH: æ·»åŠ  "Content-Type": "application/json", "Accept": "application/json"
   - DELETE: æ·»åŠ  "Accept": "application/json" ï¼ˆå¿…é¡»æ˜¯å®Œæ•´çš„"application/json"ï¼‰

2. åŸºäºŽè®¤è¯è¦æ±‚çš„Headersï¼š
   - Bearer Tokenè®¤è¯: æ·»åŠ  "Authorization": "Bearer ${AUTH_TOKEN}"
   - API Keyè®¤è¯: æ·»åŠ  "X-API-Key": "${API_KEY}" æˆ–ç›¸åº”header
   - Basic Authè®¤è¯: æ·»åŠ  "Authorization": "Basic ${BASIC_CREDENTIALS}"
   - æ— è®¤è¯è¦æ±‚: åªæ·»åŠ åŸºæœ¬çš„Accept/Content-Type headers

3. åŸºäºŽè¯·æ±‚ä½“ç±»åž‹çš„Headersï¼š
   - JSONè¯·æ±‚ä½“: "Content-Type": "application/json"
   - æ³¨æ„ï¼šbodyå­—æ®µå¿…é¡»å§‹ç»ˆæ˜¯JSONå¯¹è±¡æ ¼å¼ï¼Œä¸è¦ç”ŸæˆURLç¼–ç å­—ç¬¦ä¸²

4. è´Ÿå‘æµ‹è¯•çš„Headersç­–ç•¥ï¼š
   - ç¼ºå¤±è®¤è¯headers (è¿”å›ž401/403)
   - é”™è¯¯çš„Content-Type (è¿”å›ž415)
   - æ— æ•ˆçš„Acceptå¤´ (è¿”å›ž406)
   - å…¶ä»–æƒ…å†µå¯ä»¥ä¸ºç©º

5. å‚æ•°ç”Ÿæˆæ™ºèƒ½è§„åˆ™ï¼š
   - è·¯å¾„å‚æ•°ï¼šå¦‚æžœAPIè·¯å¾„åŒ…å«å ä½ç¬¦(å¦‚{category_id})ï¼Œpathå­—æ®µä¿æŒåŽŸæ ·åŒ…å«å ä½ç¬¦ï¼Œå®žé™…å€¼æ”¾åœ¨path_paramsä¸­
     ç¤ºä¾‹ï¼špath: "/api/v1/categories/{category_id}", path_params: {"category_id": 123}
   - GET/DELETE: å¦‚æžœè·¯å¾„åŒ…å«å ä½ç¬¦åˆ™éœ€è¦path_paramsï¼Œå¯èƒ½æœ‰query_params
   - POST/PUT/PATCH: é€šå¸¸æœ‰bodyï¼Œpath_paramsä»…åœ¨è·¯å¾„åŒ…å«å ä½ç¬¦æ—¶å­˜åœ¨ï¼Œquery_paramsè¾ƒå°‘ä½¿ç”¨
   - æžå…¶é‡è¦çš„è§„åˆ™ï¼š
     * å½“ç«¯ç‚¹æœ‰è·¯å¾„å‚æ•°æ—¶ï¼Œæ‰åŒ…å«path_paramså­—æ®µï¼Œå€¼ä¸ºå…·ä½“å‚æ•°å¯¹è±¡
     * å½“ç«¯ç‚¹æœ‰æŸ¥è¯¢å‚æ•°æ—¶ï¼Œæ‰åŒ…å«query_paramså­—æ®µï¼Œå€¼ä¸ºå…·ä½“å‚æ•°å¯¹è±¡
     * å½“ç«¯ç‚¹æ²¡æœ‰å¯¹åº”å‚æ•°æ—¶ï¼Œç»å¯¹ä¸è¦åŒ…å«è¯¥å­—æ®µï¼ˆä¸è¦è®¾ä¸ºnullã€{}ã€""æˆ–ä»»ä½•ç©ºå€¼ï¼‰
     * ç¤ºä¾‹ï¼šPOST /api/v1/auth/register åªéœ€è¦ bodyï¼Œä¸è¦åŒ…å« path_params æˆ– query_params
     * ç¤ºä¾‹ï¼šGET /api/v1/categories/{id} éœ€è¦ path_params: {"id": 1}ï¼Œä¸åŒ…å« query_params
     * ç¤ºä¾‹ï¼šGET /api/v1/products?limit=10 éœ€è¦ query_params: {"limit": 10}ï¼Œä¸åŒ…å« path_params

é‡è¦ï¼š
- ç›´æŽ¥è¿”å›žJSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–markdownæ ‡è®°
- ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œä¸è¦åŒ…å«æ³¨é‡Š
- å­—ç¬¦ä¸²ä½¿ç”¨åŒå¼•å·ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦
- Headerså¿…é¡»åŸºäºŽä¸Šè¿°è§„åˆ™æ™ºèƒ½ç”Ÿæˆï¼Œä¸è¦éšæ„è®¾ç½®
- Tagså¿…é¡»åŸºäºŽä¸Šè¿°è§„åˆ™æ™ºèƒ½ç”Ÿæˆï¼Œç»å¯¹ä¸èƒ½ä¸ºç©ºæ•°ç»„[]
- bodyå­—æ®µæ ¼å¼è¦æ±‚ï¼š
  * bodyå¿…é¡»æ˜¯JSONå¯¹è±¡æ ¼å¼ï¼Œä¾‹å¦‚ï¼š{"username": "test", "password": "123456"}
  * ç»å¯¹ä¸è¦ç”ŸæˆURLç¼–ç å­—ç¬¦ä¸²ï¼Œå¦‚ï¼š"username=test&password=123456"
  * å¦‚æžœä¸éœ€è¦bodyï¼Œåˆ™ä¸åŒ…å«bodyå­—æ®µï¼ˆä¸è¦è®¾ä¸ºnullï¼‰
- å‚æ•°å­—æ®µè§„åˆ™ï¼š
  * æœ‰è·¯å¾„å‚æ•°æ—¶æ‰åŒ…å«path_paramså­—æ®µï¼ˆä¸è¦è®¾ä¸ºnullï¼‰
  * æœ‰æŸ¥è¯¢å‚æ•°æ—¶æ‰åŒ…å«query_paramså­—æ®µï¼ˆä¸è¦è®¾ä¸ºnullï¼‰
  * æ²¡æœ‰å‚æ•°æ—¶å®Œå…¨çœç•¥è¿™äº›å­—æ®µ
- æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»åŒ…å«å®Œæ•´çš„é¢„æœŸéªŒè¯ä¿¡æ¯ï¼š
  * resp_headers: å“åº”å¤´éªŒè¯
  * resp_content: å“åº”å†…å®¹æ–­è¨€
  * rules: ä¸šåŠ¡é€»è¾‘éªŒè¯è§„åˆ™"""
    
    def _build_prompt(self, endpoint: APIEndpoint) -> str:
        """Build prompt for test case generation.
        
        Args:
            endpoint: API endpoint to generate prompt for
            
        Returns:
            Formatted prompt string
        """
        # Evaluate endpoint complexity
        complexity = self._evaluate_endpoint_complexity(endpoint)
        
        # Build endpoint description
        endpoint_info = {
            "method": endpoint.method,
            "path": endpoint.path,
            "summary": endpoint.summary,
            "description": endpoint.description
        }
        
        # Add parameters info
        if endpoint.parameters:
            params_info = []
            for param in endpoint.parameters:
                param_info = {
                    "name": param.name,
                    "location": param.location,
                    "type": param.type,
                    "required": param.required,
                    "description": param.description
                }
                if param.param_schema:
                    param_info["schema"] = param.param_schema
                params_info.append(param_info)
            endpoint_info["parameters"] = params_info
        
        # Add request body info
        if endpoint.request_body:
            endpoint_info["requestBody"] = endpoint.request_body
        
        # Add response info
        if endpoint.responses:
            endpoint_info["responses"] = endpoint.responses
        
        # Analyze headers recommendations
        headers_scenarios = self.headers_analyzer.analyze_headers(endpoint)
        
        # Build complexity guidance
        complexity_guidance = f"""
**æŽ¥å£å¤æ‚åº¦åˆ†æž:**
- å¤æ‚åº¦çº§åˆ«: {complexity['complexity_level']}
- å½±å“å› ç´ : {', '.join(complexity['factors']) if complexity['factors'] else 'åŸºç¡€æŽ¥å£'}
- æŽ¨èç”Ÿæˆæ•°é‡:
  - æ€»è®¡: å»ºè®®ç”Ÿæˆ{complexity['recommended_counts']['total'][1]}ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆæœ€å°‘{complexity['recommended_counts']['total'][0]}ä¸ªï¼‰
  - æ­£å‘æµ‹è¯•: **å»ºè®®{complexity['recommended_counts']['positive'][1]}ä¸ª**ï¼ˆä¸å°‘äºŽ{complexity['recommended_counts']['positive'][0]}ä¸ªï¼‰
  - è´Ÿå‘æµ‹è¯•: **å»ºè®®{complexity['recommended_counts']['negative'][1]}ä¸ª**ï¼ˆä¸å°‘äºŽ{complexity['recommended_counts']['negative'][0]}ä¸ªï¼‰
  - è¾¹ç•Œæµ‹è¯•: å»ºè®®{complexity['recommended_counts']['boundary'][1]}ä¸ªï¼ˆè‡³å°‘{complexity['recommended_counts']['boundary'][0]}ä¸ªï¼‰

ðŸ“Œ **æŽ¨èè¦æ±‚**: è¯·ç”ŸæˆæŽ¨èæ•°é‡çš„æµ‹è¯•ç”¨ä¾‹ä»¥ç¡®ä¿å…¨é¢è¦†ç›–ã€‚å…¨é¢çš„æµ‹è¯•è¦†ç›–æ¯”èŠ‚çœtokenæ›´é‡è¦ï¼
"""

        # Build the prompt
        prompt = f"""Generate comprehensive test cases for the following API endpoint:

**Endpoint Definition:**
```json
{json.dumps(endpoint_info, indent=2)}
```

{complexity_guidance}

**Headerså»ºè®® (æ™ºèƒ½åˆ†æžç»“æžœ):**
- æ­£å‘æµ‹è¯•å»ºè®®headers: {json.dumps(headers_scenarios.get('positive', {}), indent=2)}
- è´Ÿå‘æµ‹è¯•åœºæ™¯: {list(headers_scenarios.keys())}

**å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹éªŒè¯è¦æ±‚:**
1. **çŠ¶æ€ç éªŒè¯**: å‡†ç¡®çš„HTTPçŠ¶æ€ç æœŸæœ›
2. **å“åº”å¤´éªŒè¯**: åŒ…æ‹¬Content-Typeã€Locationã€Cache-Controlç­‰
3. **å“åº”ä½“ç»“æž„éªŒè¯**: åŸºäºŽOpenAPI schemaçš„ç»“æž„éªŒè¯
4. **å“åº”å†…å®¹éªŒè¯**: å…·ä½“å­—æ®µå€¼ã€æ ¼å¼ã€ä¸šåŠ¡é€»è¾‘éªŒè¯
5. **æ€§èƒ½éªŒè¯**: å“åº”æ—¶é—´æœŸæœ›
6. **ä¸šåŠ¡è§„åˆ™éªŒè¯**: æ•°æ®ä¸€è‡´æ€§ã€æƒé™æŽ§åˆ¶ç­‰

**Required Test Case JSON Schema:**
```json
{json.dumps(self._test_case_schema, indent=2)}
```

è¯·æ ¹æ®æŽ¥å£å¤æ‚åº¦ç”Ÿæˆç›¸åº”æ•°é‡çš„é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ã€‚æ¯ä¸ªç”¨ä¾‹éƒ½åº”è¯¥æœ‰æ˜Žç¡®çš„æµ‹è¯•ç›®çš„ï¼Œé¿å…é‡å¤æˆ–æ— æ„ä¹‰çš„æµ‹è¯•ã€‚

âš ï¸ **å…³é”®æé†’**:
1. **å¿…é¡»ç”Ÿæˆ**è¶³å¤Ÿçš„æ­£å‘æµ‹è¯•ç”¨ä¾‹ï¼šè‡³å°‘{complexity['recommended_counts']['positive'][0]}ä¸ªï¼ŒæŽ¨è{complexity['recommended_counts']['positive'][1]}ä¸ª
2. **å¿…é¡»ç”Ÿæˆ**è¶³å¤Ÿçš„è´Ÿå‘æµ‹è¯•ç”¨ä¾‹ï¼šè‡³å°‘{complexity['recommended_counts']['negative'][0]}ä¸ªï¼ŒæŽ¨è{complexity['recommended_counts']['negative'][1]}ä¸ª
3. æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
4. ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹åº”è¯¥åŒ…å«å®Œæ•´çš„é¢„æœŸéªŒè¯ï¼Œä¸ä»…ä»…æ˜¯çŠ¶æ€ç ï¼Œè¿˜è¦åŒ…æ‹¬å“åº”å¤´ã€å“åº”å†…å®¹ã€ä¸šåŠ¡è§„åˆ™ç­‰å…¨é¢çš„éªŒè¯
5. è¿”å›žæ ¼å¼å¿…é¡»æ˜¯JSONæ•°ç»„ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹ä¹Ÿè¦ç”¨ [...] åŒ…è£…

ðŸ”¥ **æœ€é‡è¦ï¼šç¡®ä¿æ­£å‘æµ‹è¯•ç”¨ä¾‹æ•°é‡è¾¾åˆ°è¦æ±‚ï¼ä¸è¦å°‘äºŽ{complexity['recommended_counts']['positive'][0]}ä¸ªï¼**

Return the test cases as a JSON array:"""
        
        return prompt
    
    def _parse_llm_response(self, response_content: str, endpoint: APIEndpoint) -> List[TestCase]:
        """Parse and validate LLM response.
        
        Args:
            response_content: Raw LLM response content
            endpoint: API endpoint for context
            
        Returns:
            List of validated test cases
            
        Raises:
            TestGeneratorError: If response is invalid
        """
        self.logger.file_only(f"ðŸ”„ Parsing LLM response ({len(response_content):,} characters)")
        self.logger.file_only("Extracting test cases from JSON structure", level="DEBUG")
        
        # Parse JSON response directly
        try:
            test_data = json.loads(response_content)
            self.logger.file_only(f"Successfully parsed JSON, type: {type(test_data).__name__}", level="DEBUG")
        except json.JSONDecodeError as e:
            # Log the problematic content for debugging
            self.logger.warning(f"Failed to parse JSON (will retry): {str(e)[:100]}")
            self.logger.debug(f"Raw content: {response_content[:500]}...")
            raise TestGeneratorError(f"Invalid JSON in LLM response: {e}")
        
        if not isinstance(test_data, list):
            # Special handling for non-array responses
            self.logger.file_only(f"Response is not an array, attempting to extract test cases from {type(test_data)}", level="WARNING")
            
            if isinstance(test_data, dict):
                # Try to find an array field in the response
                possible_keys = [
                    'response', 'data', 'result', 'test_cases', 'tests', 'items',
                    'testCases', 'test_case_list', 'cases', 'array', 'list',
                    'content', 'output', 'generated', 'testdata'
                ]
                
                extracted_array = None
                for key in possible_keys:
                    if key in test_data and isinstance(test_data[key], list):
                        extracted_array = test_data[key]
                        self.logger.file_only(f"Extracted test cases from '{key}' field: {len(extracted_array)} items", level="INFO")
                        break
                
                # Check for nested arrays
                if not extracted_array:
                    for key, value in test_data.items():
                        if isinstance(value, dict):
                            for nested_key in possible_keys:
                                if nested_key in value and isinstance(value[nested_key], list):
                                    extracted_array = value[nested_key]
                                    self.logger.file_only(f"Extracted test cases from nested '{key}.{nested_key}': {len(extracted_array)} items", level="INFO")
                                    break
                        if extracted_array:
                            break
                
                # Last resort: look for any array that might contain test case-like objects
                if not extracted_array:
                    for key, value in test_data.items():
                        if isinstance(value, list) and len(value) > 0:
                            # Check if first item looks like a test case
                            first_item = value[0]
                            if isinstance(first_item, dict):
                                test_indicators = ['test_id', 'id', 'method', 'path', 'name', 'description']
                                if any(indicator in first_item for indicator in test_indicators):
                                    extracted_array = value
                                    self.logger.file_only(f"Found test case-like array at '{key}': {len(value)} items", level="INFO")
                                    break
                
                if extracted_array:
                    test_data = extracted_array
                else:
                    # Check if the entire dict is a single test case
                    test_indicators = ['test_id', 'id', 'method', 'path', 'name', 'description', 'expected_status', 'status']
                    if any(indicator in test_data for indicator in test_indicators):
                        self.logger.file_only("Converting single test case object to array", level="INFO")
                        test_data = [test_data]
                    else:
                        # Log the structure for debugging
                        self.logger.error(f"Could not extract test cases from dict with keys: {list(test_data.keys())}")
                        if os.getenv("CASECRAFT_DEBUG_RESPONSE"):
                            import time
                            debug_file = f"failed_response_{int(time.time())}.json"
                            try:
                                with open(debug_file, 'w', encoding='utf-8') as f:
                                    json.dump(test_data, f, indent=2, ensure_ascii=False)
                                self.logger.file_only(f"Failed response saved to {debug_file}", level="INFO")
                            except Exception:
                                pass
                        raise TestGeneratorError(f"LLM response must be a JSON array of test cases. Got dict with keys: {list(test_data.keys())[:10]}")
            else:
                raise TestGeneratorError(f"LLM response must be a JSON array of test cases. Got {type(test_data).__name__}")
        
        # Validate and convert to TestCase objects
        test_cases = []
        for i, test_case_data in enumerate(test_data):
            try:
                # Fix body field if it's a URL-encoded string
                if 'body' in test_case_data and isinstance(test_case_data['body'], str):
                    body_str = test_case_data['body']
                    # Check if it looks like URL-encoded data
                    if '=' in body_str and '&' in body_str:
                        self.logger.file_only(f"Test case {i+1}: body is URL-encoded string, converting to JSON object", level="WARNING")
                        # Parse URL-encoded string
                        import urllib.parse
                        params = urllib.parse.parse_qs(body_str)
                        # Convert to simple dict (take first value for each key)
                        test_case_data['body'] = {k: v[0] if len(v) == 1 else v for k, v in params.items()}
                    else:
                        # Try to parse as JSON string
                        try:
                            test_case_data['body'] = json.loads(body_str)
                            self.logger.file_only(f"Test case {i+1}: body was JSON string, converted to object", level="WARNING")
                        except json.JSONDecodeError:
                            # If all else fails, wrap in an object
                            self.logger.file_only(f"Test case {i+1}: body is plain string, wrapping in object", level="WARNING")
                            test_case_data['body'] = {"data": body_str}
                
                # Validate against schema
                validate(test_case_data, self._test_case_schema)
                
                # Clean up null/empty parameters before creating TestCase
                # This ensures we don't have unnecessary null or empty dict fields
                params_to_check = ['path_params', 'query_params']
                for param_field in params_to_check:
                    if param_field in test_case_data:
                        value = test_case_data[param_field]
                        # Remove if None, empty dict, empty string, or string "null"
                        if value is None or value == {} or value == '' or value == 'null':
                            del test_case_data[param_field]
                
                # Convert to TestCase object
                test_case = TestCase(**test_case_data)
                test_cases.append(test_case)
                
            except ValidationError as e:
                raise TestGeneratorError(f"Test case {i} validation error: {e}")
            except PydanticValidationError as e:
                raise TestGeneratorError(f"Test case {i} model error: {e}")
        
        # Validate test case coverage
        self._validate_test_coverage(test_cases, endpoint)
        
        return test_cases
    
    def _validate_test_coverage(self, test_cases: List[TestCase], endpoint: APIEndpoint) -> None:
        """Validate that test cases meet coverage requirements based on endpoint complexity.
        
        Args:
            test_cases: Generated test cases
            endpoint: API endpoint
            
        Raises:
            TestGeneratorError: If coverage requirements not met
        """
        if not test_cases:
            raise TestGeneratorError("No test cases generated")
        
        # Evaluate endpoint complexity to get requirements
        complexity = self._evaluate_endpoint_complexity(endpoint)
        
        # Count test types
        positive_count = sum(1 for tc in test_cases if tc.test_type == TestType.POSITIVE)
        negative_count = sum(1 for tc in test_cases if tc.test_type == TestType.NEGATIVE)
        boundary_count = sum(1 for tc in test_cases if tc.test_type == TestType.BOUNDARY)
        total_count = len(test_cases)
        
        # Use 60% of minimum requirements as soft requirements (more lenient)
        min_positive = max(2, int(complexity['recommended_counts']['positive'][0] * 0.6))
        min_negative = max(3, int(complexity['recommended_counts']['negative'][0] * 0.6))
        min_total = max(8, int(complexity['recommended_counts']['total'][0] * 0.6))
        
        # Only error on severe deficiency
        if positive_count < min_positive:
            self.logger.warning(f"Positive test cases below recommended: {positive_count} < {complexity['recommended_counts']['positive'][0]}")
            if positive_count < 2:  # Only error if less than 2
                raise TestGeneratorError(f"At least 2 positive test cases required, got {positive_count}")
        
        if negative_count < min_negative:
            self.logger.warning(f"Negative test cases below recommended: {negative_count} < {complexity['recommended_counts']['negative'][0]}")
            if negative_count < 2:  # Only error if less than 2
                raise TestGeneratorError(f"At least 2 negative test cases required, got {negative_count}")
        
        if total_count < min_total:
            self.logger.warning(f"Total test cases below recommended: {total_count} < {complexity['recommended_counts']['total'][0]}")
            if total_count < 5:  # Only error if less than 5
                raise TestGeneratorError(f"At least 5 test cases required, got {total_count}")
        
        # Log test case distribution with complexity info
        # Note: This log is for validation only, actual generation success is logged in generate_test_cases method
        self.logger.file_only(f"Validated {total_count} test cases for {complexity['complexity_level']} endpoint ({endpoint.method} {endpoint.path}): {positive_count} positive, {negative_count} negative, {boundary_count} boundary", level="DEBUG")
        
        # Validate that each test case has required fields
        for i, test_case in enumerate(test_cases):
            if not test_case.name or not test_case.description:
                raise TestGeneratorError(f"Test case {i} missing name or description")
            
            if not test_case.status:
                raise TestGeneratorError(f"Test case {i} missing expected status code")
    
    def _get_test_case_schema(self) -> Dict[str, Any]:
        """Get JSON schema for test case validation."""
        return {
            "type": "object",
            "required": [
                "test_id",
                "name",
                "description", 
                "method",
                "path",
                "status",
                "test_type"
            ],
            "properties": {
                "test_id": {
                    "type": "integer",
                    "minimum": 1,
                    "description": "Test case ID/sequence number"
                },
                "name": {
                    "type": "string",
                    "minLength": 1,
                    "description": "Test case name"
                },
                "description": {
                    "type": "string",
                    "minLength": 1,
                    "description": "Test case description"
                },
                "method": {
                    "type": "string",
                    "pattern": "^(GET|POST|PUT|DELETE|PATCH|HEAD|OPTIONS)$",
                    "description": "HTTP method"
                },
                "path": {
                    "type": "string",
                    "minLength": 1,
                    "description": "API path"
                },
                "headers": {
                    "type": "object",
                    "description": "Request headers"
                },
                "path_params": {
                    "type": "object",
                    "description": "Path parameters"
                },
                "query_params": {
                    "type": "object",
                    "description": "Query parameters"
                },
                "body": {
                    "oneOf": [
                        {"type": "object"},
                        {"type": "null"}
                    ],
                    "description": "Request body"
                },
                "status": {
                    "type": "integer",
                    "minimum": 100,
                    "maximum": 599,
                    "description": "Expected HTTP status code"
                },
                "resp_schema": {
                    "type": "object",
                    "description": "Expected response schema"
                },
                "test_type": {
                    "type": "string",
                    "enum": ["positive", "negative", "boundary"],
                    "description": "Test case type"
                },
                "resp_headers": {
                    "type": "object",
                    "description": "Expected response headers"
                },
                "resp_content": {
                    "type": "object",
                    "description": "Expected response content assertions"
                },
                "rules": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Business logic validation rules"
                }
            },
            "additionalProperties": False
        }
    
    def _enhance_test_cases(self, test_cases: List[TestCase], endpoint: APIEndpoint) -> List[TestCase]:
        """Enhance test cases with response schemas and improved status codes.
        
        Args:
            test_cases: List of test cases to enhance
            endpoint: API endpoint for context
            
        Returns:
            Enhanced test cases
        """
        # Extract response schemas from endpoint
        response_schemas = self._extract_response_schemas(endpoint)
        
        # Ensure each test case has a proper test_id
        for i, test_case in enumerate(test_cases, 1):
            if not hasattr(test_case, 'test_id') or test_case.test_id is None:
                test_case.test_id = i
            
            status_str = str(test_case.status)
            
            # Add response schema for all cases with defined schemas
            if status_str in response_schemas:
                test_case.resp_schema = response_schemas[status_str]
            else:
                # Provide default schema based on status code
                test_case.resp_schema = self._get_default_response_schema(status_str)
            
            # Add expected response headers
            test_case.resp_headers = self._extract_response_headers(endpoint, status_str)
            
            # Add response content assertions
            content_assertions = self._extract_response_content_assertions(endpoint, status_str)
            if content_assertions:
                test_case.resp_content = content_assertions
            
            # Add business rules based on endpoint characteristics
            business_rules = self._generate_business_rules(test_case, endpoint)
            if business_rules:
                test_case.rules = business_rules
            
            # Improve status codes based on test type and error
            if test_case.test_type == TestType.NEGATIVE:
                test_case.status = self._infer_status_code(test_case, endpoint)
        
        return test_cases
    
    def _extract_response_schemas(self, endpoint: APIEndpoint) -> Dict[str, Dict[str, Any]]:
        """Extract response schemas from endpoint definition.
        
        Args:
            endpoint: API endpoint
            
        Returns:
            Map of status code to response schema
        """
        schemas = {}
        
        for status, response_def in endpoint.responses.items():
            if "content" in response_def:
                # Try to find JSON schema
                for content_type, content_def in response_def["content"].items():
                    if "json" in content_type.lower() and "schema" in content_def:
                        schema = content_def["schema"].copy()
                        # Simplify or remove long titles to save tokens
                        if "title" in schema and len(schema["title"]) > 20:
                            # Create a simple title based on status code
                            status_int = int(status) if status.isdigit() else 200
                            if 200 <= status_int < 300:
                                schema["title"] = "Success Response"
                            elif 400 <= status_int < 500:
                                schema["title"] = "Error Response"
                            else:
                                schema["title"] = f"Response {status}"
                        schemas[status] = schema
                        break
        
        return schemas
    
    def _get_default_response_schema(self, status_code: str) -> Dict[str, Any]:
        """Get default response schema based on status code.
        
        Args:
            status_code: HTTP status code as string
            
        Returns:
            Default response schema
        """
        status_int = int(status_code)
        
        # Success responses (2xx)
        if 200 <= status_int < 300:
            return {
                "type": "object",
                "additionalProperties": True
            }
        
        # Client error responses (4xx)
        elif 400 <= status_int < 500:
            return {
                "type": "object",
                "properties": {
                    "error": {"type": "string"},
                    "message": {"type": "string"},
                    "code": {"type": "string"}
                },
                "additionalProperties": True
            }
        
        # Server error responses (5xx)
        elif 500 <= status_int < 600:
            return {
                "type": "object",
                "properties": {
                    "error": {"type": "string"},
                    "message": {"type": "string"}
                },
                "additionalProperties": True
            }
        
        # Default fallback
        else:
            return {
                "type": "object",
                "additionalProperties": True
            }
    
    def _extract_response_headers(self, endpoint: APIEndpoint, status_code: str) -> Dict[str, Any]:
        """Extract expected response headers for a given status code.
        
        Args:
            endpoint: API endpoint
            status_code: HTTP status code (as string)
            
        Returns:
            Map of header name to expected value or pattern
        """
        headers = {}
        
        # Default headers for all responses
        headers["Content-Type"] = "application/json"
        
        # Extract from endpoint response definition
        if status_code in endpoint.responses:
            response_def = endpoint.responses[status_code]
            if "headers" in response_def:
                for header_name, header_def in response_def["headers"].items():
                    # Extract expected header value or pattern
                    if "schema" in header_def:
                        schema = header_def["schema"]
                        if "example" in schema:
                            headers[header_name] = schema["example"]
                        elif "default" in schema:
                            headers[header_name] = schema["default"]
                        else:
                            # Just indicate the header should be present
                            headers[header_name] = "<any>"
        
        # Add common response headers based on operation type
        if endpoint.method in ["POST", "PUT", "PATCH"] and status_code in ["200", "201"]:
            headers["Location"] = "<resource-url>"
        
        if status_code == "201":
            headers["Location"] = "<created-resource-url>"
        
        # Add cache-related headers for GET requests
        if endpoint.method == "GET" and status_code == "200":
            headers["Cache-Control"] = "max-age=300"
            headers["ETag"] = "<etag-value>"
        
        return headers
    
    def _extract_response_content_assertions(self, endpoint: APIEndpoint, status_code: str) -> Optional[Dict[str, Any]]:
        """Extract content validation assertions for response.
        
        Args:
            endpoint: API endpoint
            status_code: HTTP status code (as string)
            
        Returns:
            Content assertion rules or None
        """
        assertions = {}
        
        # Extract from response schema
        if status_code in endpoint.responses:
            response_def = endpoint.responses[status_code]
            if "content" in response_def:
                for content_type, content_def in response_def["content"].items():
                    if "json" in content_type.lower() and "schema" in content_def:
                        schema = content_def["schema"]
                        
                        # Add schema-based assertions
                        if "properties" in schema:
                            required_fields = schema.get("required", [])
                            if required_fields:
                                assertions["required_fields"] = required_fields
                        
                        # Add type assertions
                        if "type" in schema:
                            assertions["response_type"] = schema["type"]
                        
                        # Add example-based assertions
                        if "example" in schema:
                            assertions["example_match"] = schema["example"]
                        
                        # Add format assertions
                        if "format" in schema:
                            assertions["format"] = schema["format"]
                        
                        break
        
        # Add common assertions based on status code
        if status_code == "200":
            if endpoint.method == "GET":
                assertions["non_empty_response"] = True
        elif status_code == "201":
            assertions["created_resource_id"] = True
        elif status_code == "400":
            assertions["error_message"] = True
            assertions["error_code"] = True
        elif status_code == "404":
            assertions["error_message"] = "Resource not found"
        elif status_code == "422":
            assertions["validation_errors"] = True
        
        return assertions if assertions else None
    
    def _infer_status_code(self, test_case: TestCase, endpoint: APIEndpoint) -> int:
        """Infer appropriate status code based on test case details.
        
        Args:
            test_case: Test case to analyze
            endpoint: API endpoint for context
            
        Returns:
            Inferred status code
        """
        name_lower = test_case.name.lower()
        desc_lower = test_case.description.lower()
        combined = name_lower + desc_lower
        
        # Priority order for status code inference
        
        # 1. Authentication errors - 401
        if any(word in combined for word in ["æœªè®¤è¯", "æœªæŽˆæƒ", "unauthorized", "authentication", "æ— è®¤è¯", "æœªç™»å½•"]):
            return 401  # Unauthorized
        
        # 2. Permission errors - 403  
        if any(word in combined for word in ["æƒé™", "forbidden", "permission", "access denied", "ä¸å…è®¸"]):
            return 403  # Forbidden
        
        # 3. Validation errors - 422
        if any(word in combined for word in ["éªŒè¯", "validation", "constraint", "range", "æ ¼å¼é”™è¯¯", "ç±»åž‹é”™è¯¯"]):
            return 422  # Unprocessable Entity
        
        # 4. Missing required fields - 422 (not 400)
        if any(word in combined for word in ["missing", "required", "ç¼ºå°‘", "å¿…å¡«", "å¿…éœ€"]):
            return 422  # Unprocessable Entity for missing required fields
        
        # 5. Invalid parameter type/format - 422
        if any(word in combined for word in ["invalid type", "string instead", "format", "éžæ•°å­—", "éžæ•´æ•°"]):
            return 422  # Unprocessable Entity for type errors
        
        # 6. Resource not found - 404 (only for actual missing resources)
        if any(word in combined for word in ["not found", "nonexistent", "doesn't exist", "ä¸å­˜åœ¨çš„å•†å“", "æ‰¾ä¸åˆ°"]):
            # But not for invalid IDs - those should be 422
            if any(word in combined for word in ["invalid", "æ ¼å¼", "è´Ÿæ•°", "é›¶å€¼"]):
                return 422
            return 404  # Not Found
        
        # 7. Bad request - 400 (general client errors)
        if any(word in combined for word in ["bad request", "malformed", "é”™è¯¯è¯·æ±‚"]):
            return 400
        
        # For DELETE operations with path params, prefer 422 for invalid IDs
        if endpoint.method.upper() == "DELETE" and test_case.path_params:
            # If it's about invalid ID format, use 422
            if any(word in combined for word in ["invalid", "è´Ÿæ•°", "é›¶å€¼", "æ ¼å¼"]):
                return 422
            # If it's about non-existent resource, use 404
            if any(word in combined for word in ["ä¸å­˜åœ¨", "not found"]):
                return 404
        
        # Default to 422 for validation errors, 400 for others
        return test_case.status if test_case.status in [400, 401, 403, 404, 422] else 422
    
    def _evaluate_endpoint_complexity(self, endpoint: APIEndpoint) -> Dict[str, Any]:
        """Evaluate the complexity of an API endpoint.
        
        Args:
            endpoint: API endpoint to evaluate
            
        Returns:
            Dictionary with complexity metrics and recommended test case counts
        """
        complexity_score = 0
        factors = []
        
        # 1. Parameter complexity (adjusted weights)
        if endpoint.parameters:
            param_by_location = {"path": [], "query": [], "header": [], "cookie": []}
            
            for param in endpoint.parameters:
                location = param.location if hasattr(param, 'location') else param.get("in", "query")
                if location in param_by_location:
                    param_by_location[location].append(param)
            
            # Path parameters: 2 points each (more important)
            if param_by_location["path"]:
                complexity_score += len(param_by_location["path"]) * 2
                factors.append(f"{len(param_by_location['path'])} path params")
            
            # Query parameters: 1 point each (reduced from 2)
            if param_by_location["query"]:
                complexity_score += len(param_by_location["query"]) * 1
                factors.append(f"{len(param_by_location['query'])} query params")
            
            # Header parameters: 1 point each
            if param_by_location["header"]:
                complexity_score += len(param_by_location["header"]) * 1
                factors.append(f"{len(param_by_location['header'])} header params")
            
            # Cookie parameters: 0.5 points each
            if param_by_location["cookie"]:
                complexity_score += len(param_by_location["cookie"]) * 0.5
                factors.append(f"{len(param_by_location['cookie'])} cookie params")
            
            # Required parameters get extra points
            required_count = sum(1 for params in param_by_location.values() 
                                for p in params if (hasattr(p, 'required') and p.required) 
                                or (isinstance(p, dict) and p.get("required", False)))
            if required_count > 0:
                complexity_score += required_count * 0.5
        
        # 2. Request body complexity (increased weights)
        if endpoint.request_body:
            body_complexity = self._evaluate_schema_complexity(endpoint.request_body)
            
            # Dynamic grading with higher weights
            if body_complexity <= 3:
                score = 6  # Increased from ~3
                factors.append("simple request body")
            elif body_complexity <= 7:
                score = 10  # Increased from ~6
                factors.append("medium request body")
            elif body_complexity <= 15:
                score = 14  # Increased from ~9
                factors.append("complex request body")
            else:
                score = 18  # Very complex
                factors.append("very complex request body")
            
            complexity_score += score
        
        # 3. Operation type complexity (DELETE gets highest weight)
        method_upper = endpoint.method.upper()
        
        if method_upper == "DELETE":
            complexity_score += 7  # DELETE gets highest weight (2nd most tests)
            factors.append("DELETE operation (critical)")
        elif method_upper == "POST":
            complexity_score += 6  # POST is important
            factors.append("POST operation")
        elif method_upper in ["PUT", "PATCH"]:
            complexity_score += 5  # Update operations
            factors.append(f"{method_upper} operation")
        elif method_upper in ["HEAD", "OPTIONS"]:
            complexity_score += 1
            factors.append(f"{method_upper} operation")
        # GET gets 0 points
        
        # 4. Authentication requirements (enhanced detection)
        if self._requires_authentication(endpoint):
            complexity_score += 3  # Increased from 2
            factors.append("authentication required")
        
        # 5. Business criticality assessment
        business_score = self._evaluate_business_criticality(endpoint)
        if business_score > 0:
            complexity_score += business_score
            factors.append(f"business criticality (+{business_score})")
        
        # 6. Response complexity
        if endpoint.responses:
            response_count = len(endpoint.responses)
            if response_count > 5:
                complexity_score += 3
                factors.append(f"{response_count} response scenarios")
            elif response_count > 3:
                complexity_score += 2
                factors.append("multiple response types")
            elif response_count > 1:
                complexity_score += 1
        
        # 7. Data consistency requirements
        if method_upper in ["PUT", "PATCH", "DELETE"]:
            complexity_score += 1
            factors.append("data consistency check")
        
        # Calculate test counts using new enhanced logic
        return self._calculate_test_counts(complexity_score, method_upper, factors)
    
    def _requires_authentication(self, endpoint: APIEndpoint) -> bool:
        """
        Generic authentication requirement detection.
        Does not rely on specific implementations.
        """
        # 1. Check endpoint security field
        if hasattr(endpoint, 'security') and endpoint.security:
            return True
        
        # 2. Check for authentication-related parameters
        if endpoint.parameters:
            # Generic authentication parameter patterns
            auth_patterns = [
                "auth", "token", "key", "credential", "session",
                "jwt", "bearer", "oauth", "apikey", "api-key",
                "x-auth", "x-token", "x-api", "authorization"
            ]
            
            for param in endpoint.parameters:
                param_name = param.name if hasattr(param, 'name') else param.get("name", "")
                param_lower = param_name.lower().replace("-", "").replace("_", "")
                
                if any(pattern in param_lower for pattern in auth_patterns):
                    return True
        
        # 3. Check path for secured areas
        path_lower = endpoint.path.lower()
        secured_paths = ["admin", "private", "secure", "protected", "internal"]
        if any(word in path_lower for word in secured_paths):
            return True
        
        return False
    
    def _evaluate_business_criticality(self, endpoint: APIEndpoint) -> int:
        """
        Evaluate business criticality using smart analyzer.
        Replaces hardcoded patterns with intelligent inference.
        """
        # ä½¿ç”¨æ™ºèƒ½å…³é”®æ€§åˆ†æžå™¨æ›¿ä»£ç¡¬ç¼–ç 
        return self.criticality_analyzer.analyze(endpoint)
    
    def _calculate_test_counts(self, complexity_score: int, method: str, factors: list) -> dict:
        """
        Calculate test counts based on complexity and method using constants.
        Ensures DELETE gets 2nd most test cases.
        """
        
        # ä½¿ç”¨å¸¸é‡ä¸­çš„æ–¹æ³•åŸºå‡†æ•°é‡
        base = METHOD_BASE_COUNTS.get(method.upper(), 12)
        
        # æ ¹æ®å¤æ‚åº¦ç­‰çº§ç¡®å®šmultiplier
        if complexity_score <= COMPLEXITY_THRESHOLDS['simple']:
            multiplier = 0.8
            level = "simple"
        elif complexity_score <= COMPLEXITY_THRESHOLDS['medium']:
            multiplier = 1.0
            level = "medium"
        else:
            multiplier = 1.3
            level = "complex"
        
        # è®¡ç®—æ€»æ•°
        total = int(base * multiplier)
        
        # èŽ·å–æµ‹è¯•ç±»åž‹æ¯”ä¾‹
        ratios = TEST_TYPE_RATIOS[level]
        
        # è®¡ç®—å„ç±»åž‹æ•°é‡
        positive = max(MIN_TEST_COUNTS['positive'], int(total * ratios['positive']))
        negative = max(MIN_TEST_COUNTS['negative'], int(total * ratios['negative']))
        boundary = max(MIN_TEST_COUNTS['boundary'], int(total * ratios['boundary']))
        
        # ç¡®ä¿æ€»æ•°åŒ¹é…
        calculated_total = positive + negative + boundary
        if calculated_total != total:
            diff = total - calculated_total
            if diff > 0:
                # å¢žåŠ è´Ÿå‘æµ‹è¯•æ•°é‡
                negative += diff
            else:
                # å‡å°‘è´Ÿå‘æµ‹è¯•æ•°é‡
                negative = max(MIN_TEST_COUNTS['negative'], negative + diff)
        
        # åº”ç”¨æœ€å¤§å€¼çº¦æŸ
        positive = min(positive, MAX_TEST_COUNTS['positive'])
        negative = min(negative, MAX_TEST_COUNTS['negative'])
        boundary = min(boundary, MAX_TEST_COUNTS['boundary'])
        total = min(positive + negative + boundary, MAX_TEST_COUNTS['total'])
        
        return {
            "complexity_score": complexity_score,
            "complexity_level": level,
            "factors": factors,
            "recommended_counts": {
                "total": (total, total),
                "positive": (positive, positive),
                "negative": (negative, negative),
                "boundary": (boundary, boundary)
            }
        }
    
    def _evaluate_schema_complexity(self, schema: Dict[str, Any]) -> int:
        """Evaluate the complexity of a JSON schema.
        
        Args:
            schema: JSON schema to evaluate
            
        Returns:
            Complexity score
        """
        score = 0
        
        if isinstance(schema, dict):
            # Check for content types
            if "content" in schema:
                for content_type, content_schema in schema.get("content", {}).items():
                    if "schema" in content_schema:
                        score += self._evaluate_schema_complexity(content_schema["schema"])
            
            # Check for object properties
            if schema.get("type") == "object":
                properties = schema.get("properties", {})
                score += len(properties)
                
                # Check for required fields
                required = schema.get("required", [])
                score += len(required)
                
                # Check for nested objects
                for prop_schema in properties.values():
                    if isinstance(prop_schema, dict) and prop_schema.get("type") == "object":
                        score += 2  # Nested objects add complexity
                    elif isinstance(prop_schema, dict) and prop_schema.get("type") == "array":
                        score += 1  # Arrays add some complexity
            
            # Check for arrays
            elif schema.get("type") == "array":
                score += 2
                if "items" in schema:
                    score += self._evaluate_schema_complexity(schema["items"])
        
        return score
    
    def _generate_business_rules(self, test_case: TestCase, endpoint: APIEndpoint) -> List[str]:
        """Generate business logic validation rules for a test case.
        
        Args:
            test_case: Test case to generate rules for
            endpoint: API endpoint context
            
        Returns:
            List of business rule descriptions
        """
        rules = []
        
        # Rules based on HTTP method
        if endpoint.method == "POST" and test_case.test_type == TestType.POSITIVE:
            rules.append("åˆ›å»ºçš„èµ„æºåº”å…·æœ‰å”¯ä¸€ID")
            rules.append("å“åº”åº”åŒ…å«èµ„æºä½ç½®")
        
        elif endpoint.method == "PUT" and test_case.test_type == TestType.POSITIVE:
            rules.append("æ›´æ–°çš„èµ„æºåº”ä¿æŒæ•°æ®å®Œæ•´æ€§")
            rules.append("ç‰ˆæœ¬å·æˆ–æ—¶é—´æˆ³åº”è¢«æ›´æ–°")
        
        elif endpoint.method == "DELETE" and test_case.test_type == TestType.POSITIVE:
            rules.append("èµ„æºåº”è¢«æ ‡è®°ä¸ºå·²åˆ é™¤æˆ–ç§»é™¤")
            rules.append("åŽç»­çš„GETè¯·æ±‚åº”è¿”å›ž404")
        
        elif endpoint.method == "GET" and test_case.test_type == TestType.POSITIVE:
            rules.append("å“åº”æ•°æ®åº”ä¸Žæ•°æ®åº“ä¿æŒä¸€è‡´")
            if "list" in endpoint.path.lower() or "search" in endpoint.path.lower():
                rules.append("åˆ†é¡µåº”è¢«æ­£ç¡®å¤„ç†")
                rules.append("ç»“æžœåº”åŒ¹é…è¿‡æ»¤æ¡ä»¶")
        
        # Rules based on authentication
        has_auth = any(p.name.lower() in ["authorization", "api-key", "x-api-key"] 
                      for p in (endpoint.parameters or []))
        
        if has_auth and test_case.test_type == TestType.NEGATIVE:
            if "unauthorized" in test_case.description.lower():
                rules.append("æ— æœ‰æ•ˆè®¤è¯æ—¶åº”æ‹’ç»è®¿é—®")
            elif "forbidden" in test_case.description.lower():
                rules.append("åº”éªŒè¯ç”¨æˆ·æƒé™")
        
        # Rules based on path parameters
        if test_case.path_params and "{id}" in endpoint.path:
            if test_case.test_type == TestType.NEGATIVE:
                rules.append("æ— æ•ˆçš„IDæ ¼å¼åº”è¢«æ‹’ç»")
                rules.append("ä¸å­˜åœ¨çš„IDåº”è¿”å›žé€‚å½“çš„é”™è¯¯")
            else:
                rules.append("IDåº”å¼•ç”¨å­˜åœ¨çš„èµ„æº")
        
        # Rules for validation scenarios
        if test_case.test_type == TestType.NEGATIVE and "validation" in test_case.description.lower():
            rules.append("è¾“å…¥éªŒè¯é”™è¯¯åº”è¢«æ¸…æ™°æè¿°")
            rules.append("é”™è¯¯å“åº”åº”åŒ…å«å­—æ®µçº§åˆ«çš„é”™è¯¯ä¿¡æ¯")
        
        # Rules for boundary cases
        if test_case.test_type == TestType.BOUNDARY:
            rules.append("è¾¹ç•Œå€¼åº”è¢«ä¼˜é›…åœ°å¤„ç†")
            rules.append("ç³»ç»Ÿé™åˆ¶åº”è¢«éµå®ˆ")
        
        return rules
    
