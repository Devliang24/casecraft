# =====================================================
# CaseCraft 多 LLM 提供商配置示例
# =====================================================
# 将此文件复制为 .env 并填写实际的 API 密钥
# cp .env.example .env
# =====================================================

# ===== 提供商选择（必须指定，无默认值）=====
# 选项1: 单提供商模式（使用一个提供商处理所有请求）
# CASECRAFT_PROVIDER=glm

# 选项2: 多提供商模式（并发使用多个提供商）
CASECRAFT_PROVIDERS=glm,qwen,kimi,local

# 选项3: 通过命令行参数指定
# casecraft generate api.json --provider glm
# casecraft generate api.json --providers glm,qwen,kimi

# =====================================================
# GLM (智谱清言) 配置
# =====================================================
# 模型选择: glm-4.5-airx, glm-4.5-air, glm-4.5-flash
CASECRAFT_GLM_MODEL=glm-4.5-airx
CASECRAFT_GLM_API_KEY=YOUR_GLM_API_KEY_HERE
CASECRAFT_GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4
CASECRAFT_GLM_TIMEOUT=60
CASECRAFT_GLM_MAX_RETRIES=3
CASECRAFT_GLM_TEMPERATURE=0.7
CASECRAFT_GLM_STREAM=false
CASECRAFT_GLM_THINK=false
# GLM 只支持单并发
CASECRAFT_GLM_WORKERS=1

# =====================================================
# Qwen (通义千问) 配置
# =====================================================
# 模型选择: qwen-max, qwen-plus, qwen-turbo
CASECRAFT_QWEN_MODEL=qwen-max
CASECRAFT_QWEN_API_KEY=YOUR_QWEN_API_KEY_HERE
CASECRAFT_QWEN_BASE_URL=https://dashscope.aliyuncs.com/api/v1
CASECRAFT_QWEN_TIMEOUT=60
CASECRAFT_QWEN_MAX_RETRIES=3
CASECRAFT_QWEN_TEMPERATURE=0.7
CASECRAFT_QWEN_STREAM=false
# Qwen 支持3个并发
CASECRAFT_QWEN_WORKERS=3

# =====================================================
# Kimi (Moonshot) 配置
# =====================================================
# 模型选择: moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k
CASECRAFT_KIMI_MODEL=moonshot-v1-8k
CASECRAFT_KIMI_API_KEY=YOUR_KIMI_API_KEY_HERE
CASECRAFT_KIMI_BASE_URL=https://api.moonshot.cn/v1
CASECRAFT_KIMI_TIMEOUT=60
CASECRAFT_KIMI_MAX_RETRIES=3
CASECRAFT_KIMI_TEMPERATURE=0.7
CASECRAFT_KIMI_STREAM=false
# Kimi 支持2个并发
CASECRAFT_KIMI_WORKERS=2

# =====================================================
# Local (本地模型) 配置
# =====================================================
# 支持 Ollama 和 vLLM 等本地部署
# 模型选择取决于您本地安装的模型
CASECRAFT_LOCAL_MODEL=llama2
CASECRAFT_LOCAL_BASE_URL=http://localhost:11434
CASECRAFT_LOCAL_TIMEOUT=120
CASECRAFT_LOCAL_MAX_RETRIES=2
CASECRAFT_LOCAL_TEMPERATURE=0.7
CASECRAFT_LOCAL_STREAM=false
# 本地模型并发数根据硬件配置调整
CASECRAFT_LOCAL_WORKERS=4
# 服务类型: ollama 或 vllm
CASECRAFT_LOCAL_SERVER_TYPE=ollama

# =====================================================
# 策略配置
# =====================================================
# 提供商分配策略
# - round_robin: 轮询分配（默认）
# - random: 随机分配
# - complexity_based: 基于复杂度分配
# - manual: 手动映射（需要配合 --provider-map 参数）
CASECRAFT_PROVIDER_STRATEGY=round_robin

# =====================================================
# 故障转移配置
# =====================================================
# 是否启用故障转移
CASECRAFT_FALLBACK_ENABLED=true
# 故障转移链（按优先级排序）
CASECRAFT_FALLBACK_CHAIN=glm,qwen,kimi,local
# 故障重试次数（每个提供商）
CASECRAFT_FALLBACK_MAX_RETRIES=2

# ========================================
# Output Configuration
# ========================================
CASECRAFT_OUTPUT_DIRECTORY=test_cases
CASECRAFT_OUTPUT_ORGANIZE_BY_TAG=false
CASECRAFT_OUTPUT_FILENAME_TEMPLATE={method}_{path_slug}

# ========================================
# Processing Configuration
# ========================================
CASECRAFT_PROCESSING_WORKERS=1
CASECRAFT_PROCESSING_FORCE_REGENERATE=false
CASECRAFT_PROCESSING_DRY_RUN=false

# ========================================
# Logging Configuration
# ========================================
# Log level: DEBUG, INFO, WARNING, ERROR
CASECRAFT_LOG_LEVEL=INFO

# =====================================================
# 使用示例
# =====================================================
# 1. 单提供商模式:
#    export CASECRAFT_PROVIDER=glm
#    casecraft generate api.json
#
# 2. 多提供商并发:
#    export CASECRAFT_PROVIDERS=glm,qwen,kimi
#    casecraft generate api.json
#
# 3. 手动映射:
#    casecraft generate api.json \
#      --provider-map "/users:qwen,/products:glm,/orders:kimi"
#
# 4. 使用本地模型作为备份:
#    export CASECRAFT_FALLBACK_CHAIN=glm,qwen,local
#    casecraft generate api.json --providers glm,qwen
# =====================================================